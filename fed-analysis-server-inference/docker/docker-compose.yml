
services:

  inference-app:
    build:
      context: ../
    container_name: inference-app
    restart: unless-stopped
    volumes:
      - ../server_results:/app/server_results
      - ../server_files:/app/server_files
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"
    environment:
      PYTHONUNBUFFERED: 1
    command: sh -c "exec python run_inference.py"
