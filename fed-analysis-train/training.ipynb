{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.pipeline import Pipeline as PipelineOr\n",
    "from concrete.ml.sklearn import DecisionTreeClassifier\n",
    "from concrete.ml.common.check_inputs import check_array_and_assert\n",
    "from concrete.ml.common.utils import (\n",
    "    generate_proxy_function,\n",
    "    manage_parameters_for_pbs_errors,\n",
    "    check_there_is_no_p_error_options_in_configuration,\n",
    ")\n",
    "from concrete.ml.quantization.quantized_module import _get_inputset_generator\n",
    "from concrete.ml.deployment.fhe_client_server import FHEModelDev\n",
    "from concrete.fhe.compilation.compiler import Compiler, Configuration, DebugArtifacts, Circuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature grouping by party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "KNOWN_FEATURES: Dict[str, Dict[str, List[str]]] = {\n",
    "    # Agritech (extension / agronomy / digital footprints)\n",
    "    \"agritech\": {\n",
    "        \"numeric\": [\n",
    "            \"farm_area_ha\", \"input_cost_kes\", \"agritech_score\",\n",
    "            \"mpesa_txn_count_90d\", \"mpesa_inflow_kes_90d\", \"eo_ndvi_gs\"\n",
    "        ],\n",
    "        \"categorical\": [\"crop_primary\", \"crop_secondary\"],\n",
    "        \"boolean\": [\"irrigated\"],\n",
    "    },\n",
    "    # Bank (credit & lending)\n",
    "    \"bank\": {\n",
    "        \"numeric\": [\"loan_amount_kes\", \"tenor_months\", \"interest_rate_pct\"],\n",
    "        \"categorical\": [],\n",
    "        \"boolean\": [\"prior_default\"],  # prior credit events seen by lender/credit bureau\n",
    "    },\n",
    "    # Processor (offtake contracts & realized outputs)\n",
    "    \"processor\": {\n",
    "        \"numeric\": [\"yield_t_ha\", \"sales_kes\"],\n",
    "        \"categorical\": [],\n",
    "        \"boolean\": [\"processor_contract\"],\n",
    "    },\n",
    "    # Insurance (risk & coverage)\n",
    "    \"insurance\": {\n",
    "        \"numeric\": [\"climate_risk_index\"],\n",
    "        \"categorical\": [],\n",
    "        \"boolean\": [\"insured\"],\n",
    "    },\n",
    "    # Government (geo/soil/climate/subsidies/administrative)\n",
    "    \"government\": {\n",
    "        \"numeric\": [\"rain_mm_gs\", \"soil_quality_index\"],\n",
    "        \"categorical\": [\"county\"],\n",
    "        \"boolean\": [\"gov_subsidy\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_group_with_df(df: pd.DataFrame, group_def: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    return {\n",
    "        \"numeric\": [c for c in group_def.get(\"numeric\", []) if c in df.columns],\n",
    "        \"categorical\": [c for c in group_def.get(\"categorical\", []) if c in df.columns],\n",
    "        \"boolean\": [c for c in group_def.get(\"boolean\", []) if c in df.columns],\n",
    "    }\n",
    "\n",
    "\n",
    "def build_group_preprocessor(cols: Dict[str, List[str]]) -> ColumnTransformer:\n",
    "    num_cols = cols[\"numeric\"]\n",
    "    cat_cols = cols[\"categorical\"]\n",
    "    bool_cols = cols[\"boolean\"]\n",
    "\n",
    "    transformers = []\n",
    "    if num_cols:\n",
    "        transformers.append((\n",
    "            \"num\",\n",
    "            PipelineOr(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            ]),\n",
    "            num_cols\n",
    "        ))\n",
    "    if cat_cols:\n",
    "        transformers.append((\n",
    "            \"cat\",\n",
    "            PipelineOr(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "            ]),\n",
    "            cat_cols\n",
    "        ))\n",
    "    if bool_cols:\n",
    "        # booleans are cast to int on the dataframe; impute just-in-case\n",
    "        transformers.append((\n",
    "            \"bool\",\n",
    "            PipelineOr(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            ]),\n",
    "            bool_cols\n",
    "        ))\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.0,\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "def safe_feature_names_out(preproc: ColumnTransformer, raw_cols: List[str]) -> List[str]:\n",
    "    \"\"\"Return post-processed feature names in order; fall back to numbered columns if needed.\"\"\"\n",
    "    try:\n",
    "        names = preproc.get_feature_names_out()\n",
    "        return [str(n) for n in names]\n",
    "    except Exception:\n",
    "        # Fallback: infer width by transforming a single-row placeholder\n",
    "        df = pd.DataFrame([{c: np.nan for c in raw_cols}], columns=raw_cols)\n",
    "        arr = preproc.transform(df)\n",
    "        return [f\"col_{i}\" for i in range(arr.shape[1])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-input wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiInputModel:\n",
    "    def quantize_input(self, *X: np.ndarray):\n",
    "        self._ensure_fitted()\n",
    "        if not hasattr(self, \"input_quantizers\"):\n",
    "            raise RuntimeError(\"Input quantizers not set. Ensure the model was fit with fit_benchmark.\")\n",
    "        if sum(inp.shape[1] for inp in X) != len(self.input_quantizers):\n",
    "            raise ValueError(\"Mismatch between input dims and number of quantizers.\")\n",
    "        base = 0\n",
    "        out = []\n",
    "        for inp in X:\n",
    "            q = np.zeros_like(inp, dtype=np.int64)\n",
    "            for j in range(inp.shape[1]):\n",
    "                q[:, j] = self.input_quantizers[base + j].quant(inp[:, j])\n",
    "            out.append(q)\n",
    "            base += inp.shape[1]\n",
    "        return tuple(out) if len(out) > 1 else out[0]\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        *inputs,\n",
    "        configuration: Optional[Configuration] = None,\n",
    "        artifacts: Optional[DebugArtifacts] = None,\n",
    "        show_mlir: bool = False,\n",
    "        p_error: Optional[float] = None,\n",
    "        global_p_error: Optional[float] = None,\n",
    "        verbose: bool = False,\n",
    "        inputs_encryption_status: Optional[List[str]] = None,\n",
    "    ) -> Circuit:\n",
    "        self._ensure_fitted()\n",
    "        inputs_as_array = tuple(check_array_and_assert(inp) for inp in inputs)\n",
    "        check_there_is_no_p_error_options_in_configuration(configuration)\n",
    "        p_error, global_p_error = manage_parameters_for_pbs_errors(p_error, global_p_error)\n",
    "\n",
    "        # Default config (encrypted execution)\n",
    "        if configuration is None:\n",
    "            configuration = Configuration()\n",
    "            configuration.verbose = False\n",
    "            configuration.fhe_simulation = False\n",
    "            configuration.fhe_execution = True\n",
    "\n",
    "        # Prepare quantized representative inputset\n",
    "        q_inputs = self.quantize_input(*inputs_as_array)\n",
    "        inputset = _get_inputset_generator(q_inputs)\n",
    "\n",
    "        # Make inference accept concatenated inputs\n",
    "        if not getattr(self, \"_is_compiled\", False):\n",
    "            original = self._tree_inference\n",
    "            self._tree_inference = lambda *parts: original(np.concatenate(parts, axis=1))\n",
    "\n",
    "        # Prepare proxy with named encrypted inputs\n",
    "        if inputs_encryption_status is None:\n",
    "            inputs_encryption_status = [\"encrypted\"] * len(inputs_as_array)\n",
    "        input_names = [f\"input_{i}_encrypted\" for i in range(len(inputs_encryption_status))]\n",
    "        proxy, arg_names = generate_proxy_function(self._tree_inference, input_names)\n",
    "        statuses = {name: status for name, status in zip(arg_names.values(), inputs_encryption_status)}\n",
    "\n",
    "        compiler = Compiler(proxy, statuses)\n",
    "\n",
    "        t0 = time.time()\n",
    "        self.fhe_circuit_ = compiler.compile(\n",
    "            inputset,\n",
    "            configuration=configuration,\n",
    "            artifacts=artifacts,\n",
    "            show_mlir=show_mlir,\n",
    "            p_error=p_error,\n",
    "            global_p_error=global_p_error,\n",
    "            verbose=verbose,\n",
    "            single_precision=False,\n",
    "            fhe_simulation=False,\n",
    "            fhe_execution=True,\n",
    "        )\n",
    "        t1 = time.time()\n",
    "        self.compile_time_s_ = float(t1 - t0)\n",
    "        self._is_compiled = True\n",
    "        self.configuration = configuration\n",
    "        return self.fhe_circuit_\n",
    "\n",
    "    def _ensure_fitted(self):\n",
    "        if not hasattr(self, \"_is_fitted\") or not self._is_fitted:\n",
    "            raise RuntimeError(\"Model is not fitted yet.\")\n",
    "\n",
    "    def evaluate_with_time(self, *X_parts: np.ndarray, y_test: np.ndarray) -> dict:\n",
    "        \"\"\"Evaluate plaintext and FHE with timing.\"\"\"\n",
    "        self._ensure_fitted()\n",
    "\n",
    "        # Plaintext\n",
    "        X_concat = np.concatenate(X_parts, axis=1)\n",
    "        t0 = time.time()\n",
    "        y_pred_plain = self.predict(X_concat)\n",
    "        t1 = time.time()\n",
    "        plain_elapsed = float(t1 - t0)\n",
    "        n = len(X_concat)\n",
    "        plain_latency = plain_elapsed / max(1, n)\n",
    "\n",
    "        plain = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred_plain),\n",
    "            \"precision\": precision_score(y_test, y_pred_plain, average=\"binary\", zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred_plain, average=\"binary\", zero_division=0),\n",
    "            \"f1_score\": f1_score(y_test, y_pred_plain, average=\"binary\", zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(y_test, y_pred_plain).tolist(),\n",
    "            \"total_time_s\": plain_elapsed,\n",
    "            \"latency_per_sample_s\": plain_latency,\n",
    "        }\n",
    "\n",
    "        # FHE\n",
    "        if not hasattr(self, \"fhe_circuit_\") or not self._is_compiled:\n",
    "            raise RuntimeError(\"Compile before FHE evaluation.\")\n",
    "\n",
    "        q_parts = self.quantize_input(*X_parts)\n",
    "        if not isinstance(q_parts, tuple):\n",
    "            q_parts = (q_parts,)\n",
    "\n",
    "        fhe_preds = []\n",
    "        fhe_total = 0.0\n",
    "        for i in range(q_parts[0].shape[0]):\n",
    "            sample = tuple(part[i].reshape(1, -1) for part in q_parts)\n",
    "            t0 = time.time()\n",
    "            out = self.fhe_circuit_.encrypt_run_decrypt(*sample)\n",
    "            t1 = time.time()\n",
    "            fhe_total += (t1 - t0)\n",
    "            label = int(np.argmax(out.squeeze()))\n",
    "            fhe_preds.append(label)\n",
    "\n",
    "        fhe_preds = np.array(fhe_preds)\n",
    "        fhe_latency = fhe_total / max(1, len(fhe_preds))\n",
    "        fhe = {\n",
    "            \"accuracy\": accuracy_score(y_test, fhe_preds),\n",
    "            \"precision\": precision_score(y_test, fhe_preds, average=\"binary\", zero_division=0),\n",
    "            \"recall\": recall_score(y_test, fhe_preds, average=\"binary\", zero_division=0),\n",
    "            \"f1_score\": f1_score(y_test, fhe_preds, average=\"binary\", zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(y_test, fhe_preds).tolist(),\n",
    "            \"total_time_s\": fhe_total,\n",
    "            \"latency_per_sample_s\": fhe_latency,\n",
    "            \"compile_time_s\": float(getattr(self, \"compile_time_s_\", 0.0)),\n",
    "        }\n",
    "        return {\"plaintext_metrics\": plain, \"fhe_metrics\": fhe}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputDecisionTreeClassifier(MultiInputModel, DecisionTreeClassifier):\n",
    "    \"\"\"DecisionTree with multi-input compile/eval mixin.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class MultiInputsFHEModelDev(FHEModelDev):\n",
    "    \"\"\"Save with original DecisionTreeClassifier class for portability.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        model = copy.copy(self.model)\n",
    "        model.__class__ = DecisionTreeClassifier\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_party_from_parts(X_concat: np.ndarray, parts: List[np.ndarray]) -> Tuple[np.ndarray, ...]:\n",
    "    sizes = [p.shape[1] for p in parts]\n",
    "    splits = np.cumsum(sizes)[:-1]\n",
    "    return tuple(np.hsplit(X_concat, splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DecisionTree (max_depth=3, class_weight='balanced')...\n",
      "Compiling to FHE for parties: ['agritech', 'bank', 'processor', 'insurance', 'government'] ...\n",
      "Evaluating on test set (plaintext vs encrypted)...\n",
      "\n",
      "Saving deployment artifacts to: output/model\n",
      "\n",
      "Done.\n",
      "{\n",
      "  \"timing\": {\n",
      "    \"train_time_s\": 0.1332550048828125,\n",
      "    \"compile_time_s\": 2.080226182937622,\n",
      "    \"plaintext_total_time_s\": 0.002156972885131836,\n",
      "    \"plaintext_latency_per_sample_s\": 4.313945770263672e-06,\n",
      "    \"fhe_total_time_s\": 146.22653436660767,\n",
      "    \"fhe_latency_per_sample_s\": 0.2924530687332153\n",
      "  },\n",
      "  \"plaintext\": {\n",
      "    \"accuracy\": 0.734,\n",
      "    \"precision\": 0.13043478260869565,\n",
      "    \"recall\": 0.5806451612903226,\n",
      "    \"f1_score\": 0.21301775147928992\n",
      "  },\n",
      "  \"fhe\": {\n",
      "    \"accuracy\": 0.734,\n",
      "    \"precision\": 0.13043478260869565,\n",
      "    \"recall\": 0.5806451612903226,\n",
      "    \"f1_score\": 0.21301775147928992\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'parties': ['agritech', 'bank', 'processor', 'insurance', 'government'],\n",
       " 'target': 'default_or_claim',\n",
       " 'train_size': 2000,\n",
       " 'test_size': 500,\n",
       " 'max_depth': 3,\n",
       " 'timing': {'train_time_s': 0.1332550048828125,\n",
       "  'compile_time_s': 2.080226182937622,\n",
       "  'plaintext_total_time_s': 0.002156972885131836,\n",
       "  'plaintext_latency_per_sample_s': 4.313945770263672e-06,\n",
       "  'fhe_total_time_s': 146.22653436660767,\n",
       "  'fhe_latency_per_sample_s': 0.2924530687332153},\n",
       " 'metrics': {'plaintext_metrics': {'accuracy': 0.734,\n",
       "   'precision': 0.13043478260869565,\n",
       "   'recall': 0.5806451612903226,\n",
       "   'f1_score': 0.21301775147928992,\n",
       "   'confusion_matrix': [[349, 120], [13, 18]],\n",
       "   'total_time_s': 0.002156972885131836,\n",
       "   'latency_per_sample_s': 4.313945770263672e-06},\n",
       "  'fhe_metrics': {'accuracy': 0.734,\n",
       "   'precision': 0.13043478260869565,\n",
       "   'recall': 0.5806451612903226,\n",
       "   'f1_score': 0.21301775147928992,\n",
       "   'confusion_matrix': [[349, 120], [13, 18]],\n",
       "   'total_time_s': 146.22653436660767,\n",
       "   'latency_per_sample_s': 0.2924530687332153,\n",
       "   'compile_time_s': 2.080226182937622}},\n",
       " 'group_columns': {'agritech': {'numeric': ['farm_area_ha',\n",
       "    'input_cost_kes',\n",
       "    'agritech_score',\n",
       "    'mpesa_txn_count_90d',\n",
       "    'mpesa_inflow_kes_90d',\n",
       "    'eo_ndvi_gs'],\n",
       "   'categorical': ['crop_primary', 'crop_secondary'],\n",
       "   'boolean': ['irrigated']},\n",
       "  'bank': {'numeric': ['loan_amount_kes', 'tenor_months', 'interest_rate_pct'],\n",
       "   'categorical': [],\n",
       "   'boolean': ['prior_default']},\n",
       "  'processor': {'numeric': ['yield_t_ha', 'sales_kes'],\n",
       "   'categorical': [],\n",
       "   'boolean': ['processor_contract']},\n",
       "  'insurance': {'numeric': ['climate_risk_index'],\n",
       "   'categorical': [],\n",
       "   'boolean': ['insured']},\n",
       "  'government': {'numeric': ['rain_mm_gs', 'soil_quality_index'],\n",
       "   'categorical': ['county'],\n",
       "   'boolean': ['gov_subsidy']}},\n",
       " 'preprocessors': {'agritech': '/Users/elvin/Development/llm/dissertation-dev/fed-analysis-train/output/model/preprocessor_agritech.pkl',\n",
       "  'bank': '/Users/elvin/Development/llm/dissertation-dev/fed-analysis-train/output/model/preprocessor_bank.pkl',\n",
       "  'processor': '/Users/elvin/Development/llm/dissertation-dev/fed-analysis-train/output/model/preprocessor_processor.pkl',\n",
       "  'insurance': '/Users/elvin/Development/llm/dissertation-dev/fed-analysis-train/output/model/preprocessor_insurance.pkl',\n",
       "  'government': '/Users/elvin/Development/llm/dissertation-dev/fed-analysis-train/output/model/preprocessor_government.pkl'},\n",
       " 'features': {'party_order': ['agritech',\n",
       "   'bank',\n",
       "   'processor',\n",
       "   'insurance',\n",
       "   'government'],\n",
       "  'total_feature_width': 90,\n",
       "  'global_feature_names_out': ['agritech:farm_area_ha',\n",
       "   'agritech:input_cost_kes',\n",
       "   'agritech:agritech_score',\n",
       "   'agritech:mpesa_txn_count_90d',\n",
       "   'agritech:mpesa_inflow_kes_90d',\n",
       "   'agritech:eo_ndvi_gs',\n",
       "   'agritech:crop_primary_beans',\n",
       "   'agritech:crop_primary_coffee',\n",
       "   'agritech:crop_primary_dairy-fodder',\n",
       "   'agritech:crop_primary_horticulture',\n",
       "   'agritech:crop_primary_maize',\n",
       "   'agritech:crop_primary_millet',\n",
       "   'agritech:crop_primary_potato',\n",
       "   'agritech:crop_primary_rice',\n",
       "   'agritech:crop_primary_sorghum',\n",
       "   'agritech:crop_primary_sugarcane',\n",
       "   'agritech:crop_primary_tea',\n",
       "   'agritech:crop_primary_wheat',\n",
       "   'agritech:crop_secondary_beans',\n",
       "   'agritech:crop_secondary_coffee',\n",
       "   'agritech:crop_secondary_dairy-fodder',\n",
       "   'agritech:crop_secondary_horticulture',\n",
       "   'agritech:crop_secondary_maize',\n",
       "   'agritech:crop_secondary_millet',\n",
       "   'agritech:crop_secondary_potato',\n",
       "   'agritech:crop_secondary_rice',\n",
       "   'agritech:crop_secondary_sorghum',\n",
       "   'agritech:crop_secondary_sugarcane',\n",
       "   'agritech:crop_secondary_tea',\n",
       "   'agritech:crop_secondary_wheat',\n",
       "   'agritech:irrigated',\n",
       "   'bank:loan_amount_kes',\n",
       "   'bank:tenor_months',\n",
       "   'bank:interest_rate_pct',\n",
       "   'bank:prior_default',\n",
       "   'processor:yield_t_ha',\n",
       "   'processor:sales_kes',\n",
       "   'processor:processor_contract',\n",
       "   'insurance:climate_risk_index',\n",
       "   'insurance:insured',\n",
       "   'government:rain_mm_gs',\n",
       "   'government:soil_quality_index',\n",
       "   'government:county_Baringo',\n",
       "   'government:county_Bomet',\n",
       "   'government:county_Bungoma',\n",
       "   'government:county_Busia',\n",
       "   'government:county_Elgeyo-Marakwet',\n",
       "   'government:county_Embu',\n",
       "   'government:county_Garissa',\n",
       "   'government:county_Homa Bay',\n",
       "   'government:county_Isiolo',\n",
       "   'government:county_Kajiado',\n",
       "   'government:county_Kakamega',\n",
       "   'government:county_Kericho',\n",
       "   'government:county_Kiambu',\n",
       "   'government:county_Kilifi',\n",
       "   'government:county_Kirinyaga',\n",
       "   'government:county_Kisii',\n",
       "   'government:county_Kisumu',\n",
       "   'government:county_Kitui',\n",
       "   'government:county_Kwale',\n",
       "   'government:county_Laikipia',\n",
       "   'government:county_Lamu',\n",
       "   'government:county_Machakos',\n",
       "   'government:county_Makueni',\n",
       "   'government:county_Mandera',\n",
       "   'government:county_Marsabit',\n",
       "   'government:county_Meru',\n",
       "   'government:county_Migori',\n",
       "   'government:county_Mombasa',\n",
       "   \"government:county_Murang'a\",\n",
       "   'government:county_Nairobi',\n",
       "   'government:county_Nakuru',\n",
       "   'government:county_Nandi',\n",
       "   'government:county_Narok',\n",
       "   'government:county_Nyamira',\n",
       "   'government:county_Nyandarua',\n",
       "   'government:county_Nyeri',\n",
       "   'government:county_Samburu',\n",
       "   'government:county_Siaya',\n",
       "   'government:county_Taita-Taveta',\n",
       "   'government:county_Tana River',\n",
       "   'government:county_Tharaka-Nithi',\n",
       "   'government:county_Trans Nzoia',\n",
       "   'government:county_Turkana',\n",
       "   'government:county_Uasin Gishu',\n",
       "   'government:county_Vihiga',\n",
       "   'government:county_Wajir',\n",
       "   'government:county_West Pokot',\n",
       "   'government:gov_subsidy'],\n",
       "  'per_party_features': {'agritech': {'slice': [0, 31],\n",
       "    'feature_names_out': ['farm_area_ha',\n",
       "     'input_cost_kes',\n",
       "     'agritech_score',\n",
       "     'mpesa_txn_count_90d',\n",
       "     'mpesa_inflow_kes_90d',\n",
       "     'eo_ndvi_gs',\n",
       "     'crop_primary_beans',\n",
       "     'crop_primary_coffee',\n",
       "     'crop_primary_dairy-fodder',\n",
       "     'crop_primary_horticulture',\n",
       "     'crop_primary_maize',\n",
       "     'crop_primary_millet',\n",
       "     'crop_primary_potato',\n",
       "     'crop_primary_rice',\n",
       "     'crop_primary_sorghum',\n",
       "     'crop_primary_sugarcane',\n",
       "     'crop_primary_tea',\n",
       "     'crop_primary_wheat',\n",
       "     'crop_secondary_beans',\n",
       "     'crop_secondary_coffee',\n",
       "     'crop_secondary_dairy-fodder',\n",
       "     'crop_secondary_horticulture',\n",
       "     'crop_secondary_maize',\n",
       "     'crop_secondary_millet',\n",
       "     'crop_secondary_potato',\n",
       "     'crop_secondary_rice',\n",
       "     'crop_secondary_sorghum',\n",
       "     'crop_secondary_sugarcane',\n",
       "     'crop_secondary_tea',\n",
       "     'crop_secondary_wheat',\n",
       "     'irrigated']},\n",
       "   'bank': {'slice': [31, 35],\n",
       "    'feature_names_out': ['loan_amount_kes',\n",
       "     'tenor_months',\n",
       "     'interest_rate_pct',\n",
       "     'prior_default']},\n",
       "   'processor': {'slice': [35, 38],\n",
       "    'feature_names_out': ['yield_t_ha', 'sales_kes', 'processor_contract']},\n",
       "   'insurance': {'slice': [38, 40],\n",
       "    'feature_names_out': ['climate_risk_index', 'insured']},\n",
       "   'government': {'slice': [40, 90],\n",
       "    'feature_names_out': ['rain_mm_gs',\n",
       "     'soil_quality_index',\n",
       "     'county_Baringo',\n",
       "     'county_Bomet',\n",
       "     'county_Bungoma',\n",
       "     'county_Busia',\n",
       "     'county_Elgeyo-Marakwet',\n",
       "     'county_Embu',\n",
       "     'county_Garissa',\n",
       "     'county_Homa Bay',\n",
       "     'county_Isiolo',\n",
       "     'county_Kajiado',\n",
       "     'county_Kakamega',\n",
       "     'county_Kericho',\n",
       "     'county_Kiambu',\n",
       "     'county_Kilifi',\n",
       "     'county_Kirinyaga',\n",
       "     'county_Kisii',\n",
       "     'county_Kisumu',\n",
       "     'county_Kitui',\n",
       "     'county_Kwale',\n",
       "     'county_Laikipia',\n",
       "     'county_Lamu',\n",
       "     'county_Machakos',\n",
       "     'county_Makueni',\n",
       "     'county_Mandera',\n",
       "     'county_Marsabit',\n",
       "     'county_Meru',\n",
       "     'county_Migori',\n",
       "     'county_Mombasa',\n",
       "     \"county_Murang'a\",\n",
       "     'county_Nairobi',\n",
       "     'county_Nakuru',\n",
       "     'county_Nandi',\n",
       "     'county_Narok',\n",
       "     'county_Nyamira',\n",
       "     'county_Nyandarua',\n",
       "     'county_Nyeri',\n",
       "     'county_Samburu',\n",
       "     'county_Siaya',\n",
       "     'county_Taita-Taveta',\n",
       "     'county_Tana River',\n",
       "     'county_Tharaka-Nithi',\n",
       "     'county_Trans Nzoia',\n",
       "     'county_Turkana',\n",
       "     'county_Uasin Gishu',\n",
       "     'county_Vihiga',\n",
       "     'county_Wajir',\n",
       "     'county_West Pokot',\n",
       "     'gov_subsidy']}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "max_depth = 3\n",
    "\n",
    "outdir = Path(\"output/model\")\n",
    "if outdir.exists():\n",
    "    shutil.rmtree(outdir)\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# 1) Load CSV\n",
    "df = pd.read_csv(\"kenya_agri.csv\")\n",
    "target = \"default_or_claim\"\n",
    "if target not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target}' not found in CSV.\")\n",
    "\n",
    "# 2) Honor 'split' column if present\n",
    "has_split = \"split\" in df.columns\n",
    "if has_split:\n",
    "    df_train = df[df[\"split\"].astype(str).str.lower() == \"train\"].copy()\n",
    "    df_test = df[df[\"split\"].astype(str).str.lower() == \"test\"].copy()\n",
    "    if df_train.empty or df_test.empty:\n",
    "        raise ValueError(\"Found 'split' column but train/test partitions are empty or invalid.\")\n",
    "    y_train = df_train[target].astype(int).to_numpy()\n",
    "    y_test = df_test[target].astype(int).to_numpy()\n",
    "    X_train_df = df_train.drop(columns=[target, \"split\"])\n",
    "    X_test_df = df_test.drop(columns=[target, \"split\"])\n",
    "else:\n",
    "    y = df[target].astype(int).to_numpy()\n",
    "    X_df = df.drop(columns=[target])\n",
    "    X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "        X_df, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "# 3) Cast boolean-like columns to int (0/1) for stability\n",
    "for X_sub in (X_train_df, X_test_df):\n",
    "    for col in X_sub.columns:\n",
    "        vals = X_sub[col].dropna().unique()\n",
    "        if set(vals).issubset({0, 1, True, False}):\n",
    "            # Fill missing with 0 (False) before casting\n",
    "            X_sub[col] = X_sub[col].fillna(0).astype(int)\n",
    "\n",
    "# 4) Build per-party preprocessors (only for groups with at least one present column)\n",
    "party_order: List[str] = []\n",
    "group_columns: Dict[str, Dict[str, List[str]]] = {}\n",
    "preprocessors: Dict[str, ColumnTransformer] = {}\n",
    "\n",
    "# Determine from the union of train/test columns\n",
    "X_all_cols = list(set(X_train_df.columns) | set(X_test_df.columns))\n",
    "\n",
    "def intersect_df(df_like: pd.DataFrame, spec: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    return {\n",
    "        \"numeric\": [c for c in spec.get(\"numeric\", []) if c in X_all_cols],\n",
    "        \"categorical\": [c for c in spec.get(\"categorical\", []) if c in X_all_cols],\n",
    "        \"boolean\": [c for c in spec.get(\"boolean\", []) if c in X_all_cols],\n",
    "    }\n",
    "\n",
    "for party, spec in KNOWN_FEATURES.items():\n",
    "    cols = intersect_df(X_train_df, spec)\n",
    "    if any(cols.values()):\n",
    "        party_order.append(party)\n",
    "        group_columns[party] = cols\n",
    "        preprocessors[party] = build_group_preprocessor(cols)\n",
    "\n",
    "if not party_order:\n",
    "    raise RuntimeError(\"No known party features found in CSV; update KNOWN_FEATURES to match your columns.\")\n",
    "\n",
    "# 5) Fit/transform by party\n",
    "X_train_parts: List[np.ndarray] = []\n",
    "X_test_parts: List[np.ndarray] = []\n",
    "preproc_paths: Dict[str, Path] = {}\n",
    "per_party_feature_names: Dict[str, List[str]] = {}\n",
    "per_party_slices: Dict[str, List[int]] = {}\n",
    "\n",
    "offset = 0\n",
    "for party in party_order:\n",
    "    pre = preprocessors[party]\n",
    "    cols = group_columns[party][\"numeric\"] + group_columns[party][\"categorical\"] + group_columns[party][\"boolean\"]\n",
    "\n",
    "    X_tr = pre.fit_transform(X_train_df[cols])\n",
    "    X_te = pre.transform(X_test_df[cols])\n",
    "\n",
    "    names = safe_feature_names_out(pre, cols)\n",
    "    width = int(X_tr.shape[1])\n",
    "    per_party_feature_names[party] = names\n",
    "    per_party_slices[party] = [offset, offset + width]\n",
    "    offset += width\n",
    "\n",
    "    X_train_parts.append(np.asarray(X_tr, dtype=np.float32))\n",
    "    X_test_parts.append(np.asarray(X_te, dtype=np.float32))\n",
    "\n",
    "\n",
    "# Concatenate for model fit\n",
    "X_train_concat = np.concatenate(X_train_parts, axis=1)\n",
    "X_test_concat = np.concatenate(X_test_parts, axis=1)\n",
    "\n",
    "total_width = int(X_train_concat.shape[1])\n",
    "\n",
    "# 6) Train DT (balanced & shallow)\n",
    "print(f\"\\nTraining DecisionTree (max_depth={max_depth}, class_weight='balanced')...\")\n",
    "model = MultiInputDecisionTreeClassifier(\n",
    "    max_depth=max_depth,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=random_state\n",
    ")\n",
    "# Fit with fit_benchmark to populate quantizers/_tree_inference\n",
    "t0 = time.time()\n",
    "model, _ = model.fit_benchmark(X_train_concat, y_train)\n",
    "t1 = time.time()\n",
    "train_time_s = float(t1 - t0)\n",
    "\n",
    "# 7) Compile to FHE as multi-input (time it)\n",
    "print(f\"Compiling to FHE for parties: {party_order} ...\")\n",
    "def split_by_party(X_concat: np.ndarray, parts: List[np.ndarray]) -> Tuple[np.ndarray, ...]:\n",
    "    sizes = [p.shape[1] for p in parts]\n",
    "    splits = np.cumsum(sizes)[:-1]\n",
    "    return tuple(np.hsplit(X_concat, splits))\n",
    "\n",
    "train_parts_for_compile = split_by_party(X_train_concat, X_train_parts)\n",
    "enc_status = [\"encrypted\"] * len(train_parts_for_compile)\n",
    "\n",
    "model.compile(*train_parts_for_compile, inputs_encryption_status=enc_status)\n",
    "compile_time_s = float(getattr(model, \"compile_time_s_\", 0.0))\n",
    "\n",
    "# 8) Evaluate with timing\n",
    "print(\"Evaluating on test set (plaintext vs encrypted)...\")\n",
    "test_parts_for_eval = split_by_party(X_test_concat, X_test_parts)\n",
    "results = model.evaluate_with_time(*test_parts_for_eval, y_test=y_test)\n",
    "\n",
    "# 9) Save deployment and metadata\n",
    "print(\"\\nSaving deployment artifacts to:\", outdir)\n",
    "fhe_model_dev = MultiInputsFHEModelDev(outdir, model)\n",
    "fhe_model_dev.save(via_mlir=True)\n",
    "\n",
    "# 10) save preprocessors\n",
    "for party, pre in preprocessors.items():\n",
    "    p_path = outdir / f\"preprocessor_{party}.pkl\"\n",
    "    with p_path.open(\"wb\") as f:\n",
    "        pickle.dump(pre, f)\n",
    "    preproc_paths[party] = p_path\n",
    "\n",
    "# 11) Build 'features' section \n",
    "global_feature_names = []\n",
    "for party in party_order:\n",
    "    global_feature_names.extend([f\"{party}:{n}\" for n in per_party_feature_names[party]])\n",
    "\n",
    "features_section = {\n",
    "    \"party_order\": party_order,\n",
    "    \"total_feature_width\": total_width,\n",
    "    \"global_feature_names_out\": global_feature_names,\n",
    "    \"per_party_features\": {\n",
    "        party: {\n",
    "            \"slice\": per_party_slices[party],\n",
    "            \"feature_names_out\": per_party_feature_names[party],\n",
    "        }\n",
    "        for party in party_order\n",
    "    }\n",
    "}\n",
    "\n",
    "report = {\n",
    "    \"parties\": party_order,\n",
    "    \"target\": target,\n",
    "    \"train_size\": int(len(y_train)),\n",
    "    \"test_size\": int(len(y_test)),\n",
    "    \"max_depth\": int(max_depth),\n",
    "    \"timing\": {\n",
    "        \"train_time_s\": train_time_s,\n",
    "        \"compile_time_s\": compile_time_s,\n",
    "        \"plaintext_total_time_s\": results[\"plaintext_metrics\"][\"total_time_s\"],\n",
    "        \"plaintext_latency_per_sample_s\": results[\"plaintext_metrics\"][\"latency_per_sample_s\"],\n",
    "        \"fhe_total_time_s\": results[\"fhe_metrics\"][\"total_time_s\"],\n",
    "        \"fhe_latency_per_sample_s\": results[\"fhe_metrics\"][\"latency_per_sample_s\"],\n",
    "    },\n",
    "    \"metrics\": results,\n",
    "    \"group_columns\": group_columns,\n",
    "    \"preprocessors\": {p: str(path.resolve()) for p, path in preproc_paths.items()},\n",
    "    \"features\": features_section,\n",
    "}\n",
    "with (outdir / \"report.json\").open(\"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(json.dumps({\n",
    "    \"timing\": report[\"timing\"],\n",
    "    \"plaintext\": {\n",
    "        k: report[\"metrics\"][\"plaintext_metrics\"][k]\n",
    "        for k in (\"accuracy\",\"precision\",\"recall\",\"f1_score\")\n",
    "    },\n",
    "    \"fhe\": {\n",
    "        k: report[\"metrics\"][\"fhe_metrics\"][k]\n",
    "        for k in (\"accuracy\",\"precision\",\"recall\",\"f1_score\")\n",
    "    }\n",
    "}, indent=2))\n",
    "\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedanalysisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
